# What are they all thinking?

[Slides](https://docs.google.com/presentation/d/1QIVyS-6oISrvXe7nO22DDzehBO02oYPx9I2FSutbnRg/edit?usp=sharing)

## Context

This is meant to build on the previous introduction to checking for understanding, and embed the idea that checking for understanding is something that teachers can \(and should\) do _frequently_ rather than infrequently.

The main desired takeaway from this session is the Cold Call technique - so, as a trainer, try to model this when you can!

## Trainer notes

### Spot the difference \(slide 2\)

The previous session was "Do they understand what I taught?", and this is "What are they thinking?".

These are similar, but subtly different.

### Real-time vs lagged assessment \(slide 3\)

> Imagine that we're on a pirate ship.
>
> You've planned a route to an island with some treasure.
>
> Real-time assessment is sending somebody up to the crow's nest and getting periodic reports on whether you seem to be sailing in the right direction.
>
> Lagged assessment is disembarking on an island and inspecting whether it seems to be the island you thought you were going to arrive at.

This pirate ship analogy is meant to be a way in for trainees to understand the need for real-time assessment - why is it important for the captain of the pirate ship to use real-time assessment? \(As a pre-emptive tool - can avoid needlessly arriving at the wrong island.\)

This is meant to map onto the original 'spot the difference': the question, "what are they thinking?", is meant to be geared towards real-time assessment.

### _Cold Call_ to get a more representative sample \(slide 4\)

Another classic rookie teacher habit is to ask questions, "What's the difference between CSS padding and margin?", then go to a student with their hand up - who gets it correct - before moving on.

The main problem with this is that students with their hands up are not representative of the class - they will be the confident ones, who are also more likely to be the ones who understand what you're trying to explain. In other words, they give a falsely optimistic view of the whole class's level of understanding.

A quick win for trainees is to _cold call_ instead. This also helps to embed a culture of engagement and responsiveness - students know they need to pay attention if they could be called on at any moment.

### Sample data through _Targeted Questioning_ \(Slide 5\)

Targeted Questioning is essentially a series of Cold Calls, e.g. \(by way of modelling\):

* What do you think ‘strategic sample’ means?
* Why do you think open-ended questions would be useful here?
* What type of data are we trying to gather?

It's intended to:

* Ensure data comes from different groups in the class \(less confident & more confident\)
* Inject pace and energy into a lesson

Open-ended questions let you also gather insight into the thought process behind the answers.

### Targeted Questioning example \(Slide 6\)

Some characteristics to note here:

* The questions are trying to test precisely what has just been taught
* The questions _ought_ to be relatively straightforward to answer if the material is understood
* The questions are relatively similar - this helps to blitz through them at pace

### CFU \(Slide 7\)

This is mostly just a prompt to model what has been taught, checking for understanding in trainees via Cold Call and Targeted Questioning.

### Reflections \(Slide 9\)

I would typically ask them to spend 1-2 minutes thinking about this, and then invite a couple of trainees to share.

